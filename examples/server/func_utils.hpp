#include <iostream>
#include <string>
#include <vector>
#include <sstream>
#include <memory>
#include <stdexcept>
#include <cstring> 
#include <algorithm>
#include <random>

#include "json.hpp"

/**
 * Integration with functionary model: https://github.com/MeetKai/functionary
 * Based on my research: https://github.com/ggerganov/llama.cpp/issues/5588
 * 
 * A typical flow is:
 * - Step 1: user send request to model
 * - Step 2: model send back a response to user
 * - Step 3: model send back another response to function (optional)
 * - Step 4: function send its returned value to model
 * - Step 5: finally, model send final response back to user
 */

 /**
 * modify by Dataelem, Inc, support qwen and command r tools call in llama.cpp
 */

namespace llama_functionary {

// using json = nlohmann::json;
using json = nlohmann::ordered_json;

static std::string random_string(int n) {
    static const std::string str("0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz");

    std::random_device rd;
    std::mt19937 generator(rd());

    std::string result(n, ' ');

    for (int i = 0; i < n; ++i) {
        result[i] = str[generator() % str.size()];
    }

    return result;
}

// the function is generated by ChatGPT, 2024/04/11
inline bool contains_chinese(const std::string& text) {
    const unsigned char* p = reinterpret_cast<const unsigned char*>(text.c_str());
    int len = std::strlen((const char*)p);
    for (int i = 0; i < len; ++i) {
        if (p[i] >= 0xE4 && p[i] <= 0xE9) {
            if (i + 2 < len) {
                unsigned char c1 = p[i], c2 = p[i+1], c3 = p[i+2];
                if ((c1 == 0xE4 && c2 >= 0xB8 && c3 >= 0x80) ||
                    (c1 == 0xE9 && c2 <= 0xBE && c3 <= 0xBF) ||
                    (c1 > 0xE4 && c1 < 0xE9)) {
                    return true;
                }
            }
            i += 2;
        }
    }
    return false;
}

inline std::string&
ltrim(std::string& str)
{
  auto p = std::find_if(
      str.begin(), str.end(), std::not1(std::ptr_fun<int, int>(std::isspace)));
  str.erase(str.begin(), p);
  return str;
}

inline std::string&
rtrim(std::string& str)
{
  auto p = std::find_if(
      str.rbegin(), str.rend(),
      std::not1(std::ptr_fun<int, int>(std::isspace)));
  str.erase(p.base(), str.end());
  return str;
}

inline std::string&
trim(std::string& str)
{
  ltrim(rtrim(str));
  return str;
}

template <typename T>
static T json_value(const json &body, const std::string &key, const T &default_value)
{
    // Fallback null to default value
    return body.contains(key) && !body.at(key).is_null()
        ? body.value(key, default_value)
        : default_value;
}

inline std::string str_replace(const std::string & original, const std::string & search, const std::string & replacement) {
    size_t pos = original.find(search);
    if (pos != std::string::npos) {
        std::string result = original;
        result.replace(pos, search.length(), replacement);
        return result;
    }
    return original;
}

inline std::vector<std::string> str_split(std::string str, const std::string & delimiter) {
    size_t pos = 0;
    std::string token;
    std::vector<std::string> output;
    while ((pos = str.find(delimiter)) != std::string::npos) {
        token = str.substr(0, pos);
        output.push_back(token);
        str.erase(0, pos + delimiter.length());
    }
    output.push_back(str); // the rest
    return output;
}


typedef struct message {
    // "system", "user", "assistant", "tool", Assistant with tool call
    std::string role;
    std::string content;
    std::string name;
    bool has_stop = false;
    size_t tool_calls_cnt = 0;
    json tool_calls;
    message() {}
    message(json oai_json) {
        role = json_value(oai_json, "role", std::string(""));
        content = json_value(oai_json, "content", std::string(""));
        name = json_value(oai_json, "name", std::string(""));
        if (role == "assistant") {
            if (oai_json.contains("tool_calls")) {
                tool_calls_cnt = oai_json["tool_calls"].size();
                tool_calls = oai_json["tool_calls"];
            }
        }
    }
} message;

typedef struct function_param {
    std::string name;
    // type can be "string", "boolean", "number" (typescript types)
    // we do not support array for now
    std::string type;
    std::string desc;
    std::vector<json> allowed_values; // dynamic types
    bool required;
    function_param(std::string param_name, json & oai_json) {
        name = param_name;
        type = json_value(oai_json, "type", std::string());
        desc = json_value(oai_json, "description", std::string());
        if (oai_json.count("enum")) {
            allowed_values = oai_json["enum"];
        }
    }
} function_param;

typedef struct function_def {
    std::string name;
    std::string desc;
    json parameters;
    // parameters.type must always be "object"
    function_def(json & oai_json) {
        std::string type = json_value(oai_json, "type", std::string());
        if (type != "function") {
            throw std::runtime_error("Only tool type \"function\" is supported");
        }
        // function
        json inner_json = json_value(oai_json, "function", json::object());
        name = json_value(inner_json, "name", std::string());
        desc = json_value(inner_json, "description", std::string());
        // function.parameters
        parameters = json_value(inner_json, "parameters", json::object());
    }
} function_def;


inline std::string serialize_function(function_def & fn) {
    std::stringstream ss;
    if (fn.name.empty()) {
        throw std::runtime_error("Function name is empty");
    }

    ss << "### " << fn.name << "\n\n" << fn.name << ": " << fn.desc;
    auto param_str = fn.parameters.dump_style();
    bool has_chinese = contains_chinese(fn.name) || contains_chinese(fn.desc) || contains_chinese(param_str);
    if (has_chinese) {
       ss << " 输入参数：" << param_str << "\n"; 
    } else {
        ss << " Parameters: " << param_str << "\n\n";
    }
    return ss.str();
}

inline static bool adapte_oai_with_cohere_api(json & body) {
    // api refer https://docs.cohere.com/reference/chat
    // parameter align
    float p = json_value(body, "p", 0.75);
    body["top_p"] = p;

    int k = json_value(body, "k", 0);
    body["top_k"] = k;

    float temperature = json_value(body, "temperature", 0.3);
    body["temperature"] = temperature;

    auto stop = json_value(body, "stop_sequences", json::array());
    body["stop"] = stop;

    bool raw_prompt = json_value(body, "raw_prompt", false);
    if (raw_prompt) {
        body["prompt"] = body["message"];
    } else {
        json oai_messages = json::array();
        auto messages = json_value(body, "chat_history", json::array());
        for (const auto & msg: messages) {
            std::string role = json_value(msg, "role", std::string());
            json oai_msg = json::object();
            if (role.compare("USER") == 0) {
                oai_msg["role"] = "user";
                oai_msg["content"] = msg["message"];
            } else if (role.compare("CHATBOT") == 0) {
                oai_msg["role"] = "assistant";
                oai_msg["content"] = msg["message"];
            } else if (role.compare("SYSTEM") == 0) {
                oai_msg["role"] = "system";
                oai_msg["content"] = msg["message"];
            }
            oai_messages.emplace_back(oai_msg);
        }

        std::string message = json_value(body, "message", std::string());
        if (!message.empty()) {
            json oai_msg = json::object();
            oai_msg["role"] = "user";
            oai_msg["content"] = message;
            oai_messages.emplace_back(oai_msg);
        }
        body["messages"] = oai_messages;
    }
    return true;
}

///////////////////////////////////////////
inline bool adapte_oai_with_tool_call(json & body) {
    std::string model = json_value(body, "model", std::string());
    std::transform(model.begin(), model.end(), model.begin(),
                [](unsigned char c){ return std::tolower(c); });

    if (model.find("command-r") != std::string::npos) {
        adapte_oai_with_cohere_api(body);
        return true;
    }

    // only support qwen1.5 model
    if (model.find("qwen1.5") == std::string::npos) {
        return true;
    }

    // set the default parameters
    if (!body.contains("top_k")) {
        body["top_k"] = 0;
    }

    if (!body.contains("top_p")) {
        body["top_p"] = 0.8;
    }

    if (!body.contains("termperature")) {
        body["termperature"] = 0.85;
    }

    if (!body.contains("repetition_penalty")) {
        body["repetition_penalty"] = 1.1;
    }

    // Qwen Function Call Prompt Design:
    // Part I. system content: system message + Tools Def Context  + Tools Instruct Context
    // Part II. user content: user message + Function Result message
    const std::string QWEN_DEFAULT_SYSTEM = "You are a helpful assistant.";
    const std::string FUNCTIONARY_FN_PROMPT_EN = "# Tools\n\n## You have access to the following tools:\n\n";
    const std::string FUNCTIONARY_FN_PROMPT_ZH = "# 工具\n\n## 你拥有如下工具：\n\n";

    // unsigned char special_token_bytes[3] = {0xe2, 0x9c, 0xbf};
    // std::string special_token(reinterpret_cast<char*>(special_token_bytes), sizeof(special_token_bytes));

    // parse message
    std::vector<json> messages = json_value(body, "messages", json::array());
    // u: user, f: assistant with tool call infomation, t: tool_mesage, a: general assistant messsage
    // U: user message with tool call
    // support case: s ua ua Uftftfta u -> s ua ua Ua u, filter the previous ft messages
    // find the last a message, and mask the previous ft messages
    int last_a_index = -1;
    int message_size = (int)messages.size();
    for (int i = message_size - 1; i >= 0; i--) {
        std::string role = json_value(messages[i], "role", std::string());
        if (role.compare("assistant") == 0 && !messages[i].contains("tool_calls")) {
            last_a_index = i;
            break;
        }
    }

    std::vector<message> ua_messages;
    std::vector<message> ft_messages;
    std::string system_content = "";
    for (int i = 0; i < message_size; i++) {
        message msg(messages[i]);
        if (msg.role.compare("assistant") == 0) {
            if (msg.tool_calls_cnt > 0) {
                if (i > last_a_index) {
                    ft_messages.emplace_back(msg);
                }
            } else {
                ua_messages.emplace_back(msg);
            }
        }
        if (msg.role.compare("tool") == 0 ) {
            if (i > last_a_index) {
                ft_messages.emplace_back(msg);
            }
        } else if (msg.role.compare("system") == 0) {
            system_content = msg.content;
        } else if (msg.role.compare("user") == 0) {
            ua_messages.emplace_back(msg);
        }
    }
    if (system_content.empty()) {
        system_content = QWEN_DEFAULT_SYSTEM;
    }

    // Part I. Tools Def Context
    std::string tools_def_context = "";
    std::vector<json> tools = json_value(body, "tools", json::array());
    bool has_chinese_prompt = false;
    for (const auto& msg: ua_messages) {
        if (msg.role.compare("user") == 0 && contains_chinese(msg.content)) {
            has_chinese_prompt = true;
            break;
        }
    }
    
    std::string tool_names = "";
    if (!tools.empty()) {
        std::stringstream ss_fn;
        ss_fn << (has_chinese_prompt ? FUNCTIONARY_FN_PROMPT_ZH : FUNCTIONARY_FN_PROMPT_EN);
        for (auto & tool: tools) {
            function_def fn(tool);
            if (tool_names.empty()) {
                tool_names += fn.name;
            } else {
               tool_names += ("," + fn.name); 
            }
            ss_fn << serialize_function(fn);
        }
        tools_def_context = ss_fn.str();
    }

    // Part I. system message
    std::stringstream ss;
    if (!tools_def_context.empty()) {
        ss << system_content << "\n\n";
        // Part I. join Tools Instruct Context
        ss << tools_def_context << "\n";
        if (!has_chinese_prompt) {
            ss << "## When you need to call a tool, please insert the following command in your reply, which can be called zero or multiple times according to your needs:\n\n";
            ss << "✿FUNCTION✿: The tool to use, should be one of [" << tool_names << "]\n";
            ss << "✿ARGS✿: The input of the tool\n";
            ss << "✿RESULT✿: The result returned by the tool. The image needs to be rendered as ![](url)\n";
            ss << "✿RETURN✿: Reply based on tool result";
        } else {
            ss << "## 你可以在回复中插入零次、一次或多次以下命令以调用工具：\n\n";
            ss << "✿FUNCTION✿: 工具名称，必须是[" << tool_names << "]之一。\n";
            ss << "✿ARGS✿: 工具输入\n";
            ss << "✿RESULT✿: 工具结果，需将图片用![](url)渲染出来。\n";
            ss << "✿RETURN✿: 根据工具结果进行回复";
        }
        system_content = ss.str();
        auto stop = json_value(body, "stop", json::array());
        stop.emplace_back("✿RESULT✿");
        stop.emplace_back("✿RESULT✿:");
        stop.emplace_back("✿RESULT✿:\n");
        body["stop"] = stop;
    }

    ss.str("");
    ss.clear();
    bool has_ft_messages = ft_messages.size() > 0;
    json oai_messages = json::array();
    json system_msg = json::object();
    system_msg["role"] = "system";
    system_msg["content"] = system_content;
    oai_messages.emplace_back(system_msg);

    // s: system message
    // U: user message with tool call, f: assistant result with tool call
    // u: user message, t: tool call result
    // support s ua ua U only
    // Part II. user message + Function Result message
    for (size_t i = 0; i < ua_messages.size() - 1; i++) {
        json msg = json::object();
        msg["role"] = ua_messages[i].role;
        msg["content"] = ua_messages[i].content;
        oai_messages.emplace_back(msg);
    }

    if (has_ft_messages) {
        // Last Message must be use role
        message msg = ua_messages[ua_messages.size() - 1];
        ss << msg.content;
        // check ft_messages
        if (ft_messages.size() % 2 != 0) {
            return false;
        }
        ss << "\n\n\n";
        for (size_t i = 0; i < ft_messages.size() - 1; i+=2) {
            message f_msg = ft_messages[i];
            message t_msg = ft_messages[i + 1];
            std::string args_str = f_msg.tool_calls[0]["function"]["arguments"];
            if (i > 0) {
                ss << ":\n";
            }
            ss << "✿FUNCTION✿: " << t_msg.name << "\n";
            ss << "✿ARGS✿: " <<  args_str << "\n";
            ss << "✿RESULT✿: " << t_msg.content << "\n";
            ss << "✿RETURN✿";
        }
        std::string user_content = ss.str();
        json msg_ = json::object();
        msg_["role"] = "user";
        msg_["content"] = user_content;
        oai_messages.emplace_back(msg_);
    } else {
        json msg = json::object();
        msg["role"] = ua_messages[ua_messages.size() - 1].role;
        msg["content"] = ua_messages[ua_messages.size() - 1].content;
        oai_messages.emplace_back(msg);
    }

    body["messages"] = oai_messages;
    return true;
}

inline bool convert_response_to_oai_choices(json& response) {
    const std::string functionTag = "✿FUNCTION✿:";
    const std::string argsTag = "✿ARGS✿:";
    
    std::string model = json_value(response, "model", std::string());
    std::transform(model.begin(), model.end(), model.begin(),
                [](unsigned char c){ return std::tolower(c); });
    if (model.find("qwen1.5") == std::string::npos)  {
        return true;
    }

    std::string content = json_value(response["choices"][0]["message"], "content", std::string());

    // Find the function name
    size_t functionNameStart = content.find(functionTag);
    if (functionNameStart == std::string::npos) {
        return false;
    }
    
    size_t functionNameEnd = content.find('\n', functionNameStart);
    std::string functionName = content.substr(
        functionNameStart + functionTag.length(), 
        functionNameEnd - functionNameStart - functionTag.length());

    // trim the function name
    functionName = trim(functionName);
    if (functionName.empty()) {
        return false;
    }

    // Find the prefix assistant message
    std::string assistant_prefix = content.substr(0, functionNameStart);
    assistant_prefix = trim(assistant_prefix);

    // Find the arguments
    size_t argsStart = content.find(argsTag);
    size_t argsEnd = content.find("}\n", argsStart);
    std::string argsString = content.substr(
        argsStart + argsTag.length(), argsEnd + 1 - argsStart - argsTag.length());
    argsString = trim(argsString);
    if (argsString.empty()) {
        argsString = "{}";
    }

    // special process for the code interpreter arguments, process the argsString
    // Input: ```py\nprint("Hello, World!")\n```
    // Output: {"python_code": "print(\"Hello, World!\")"}
    const std::string pycodeStartTag = "```py\n";
    const std::string pycodeEndTag = "\n```";

    size_t pycodeStart = argsString.find(pycodeStartTag);
    size_t pycodeEnd = argsString.find(pycodeEndTag);
    if (pycodeStart != std::string::npos && pycodeEnd != std::string::npos) {
        std::string pycode = argsString.substr(
            pycodeStart + pycodeStartTag.length(), 
            pycodeEnd - pycodeStart - pycodeStartTag.length());
        argsString = "{\"python_code\": \"" + pycode + "\"}";
    }

    json output = json::object();
    output["role"] = "assistant";
    output["content"] = nullptr;
    output["tool_calls"] = json::array();
    output["tool_calls"].push_back(json::object());
    output["tool_calls"][0]["id"] = "call_" + random_string(25);
    output["tool_calls"][0]["function"] = json::object();
    output["tool_calls"][0]["function"]["name"] = functionName;
    output["tool_calls"][0]["function"]["arguments"] = argsString;
    output["tool_calls"][0]["type"] = "function";
    response["choices"][0]["message"] = output;
    return true;
}

} // namespace llama_functionary