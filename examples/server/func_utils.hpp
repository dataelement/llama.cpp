#include <iostream>
#include <string>
#include <vector>
#include <sstream>
#include <memory>
#include <stdexcept>
#include <cstring> 
#include <algorithm>
#include <random>
#include <ctime>
#include <iomanip>

#include "json.hpp"

/**
 * Integration with functionary model: https://github.com/MeetKai/functionary
 * Based on my research: https://github.com/ggerganov/llama.cpp/issues/5588
 * 
 * A typical flow is:
 * - Step 1: user send request to model
 * - Step 2: model send back a response to user
 * - Step 3: model send back another response to function (optional)
 * - Step 4: function send its returned value to model
 * - Step 5: finally, model send final response back to user
 */

 /**
 * modify by Dataelem, Inc, support qwen and command r tools call in llama.cpp
 */

namespace llama_functionary {

class Status {
public:
    Status(bool ok, std::string message) : ok_(ok), message_(message) {}
    Status() : ok_(true), message_("") {}
    bool ok() const { return ok_; }
    std::string message() const { return message_; }

private:
    bool ok_;
    std::string message_;
};

// using json = nlohmann::json;
using json = nlohmann::ordered_json;

static std::string random_string(int n) {
    static const std::string str("0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz");

    std::random_device rd;
    std::mt19937 generator(rd());

    std::string result(n, ' ');

    for (int i = 0; i < n; ++i) {
        result[i] = str[generator() % str.size()];
    }

    return result;
}

inline static json safe_parse_json(const std::string& jsonString, Status& status) {
    json j;
    try {
        j = json::parse(jsonString);
    } catch (nlohmann::json::parse_error& e) {
        status = Status(false, e.what());
    }
    return j;
}

// the function is generated by ChatGPT, 2024/04/11
inline bool contains_chinese(const std::string& text) {
    const unsigned char* p = reinterpret_cast<const unsigned char*>(text.c_str());
    int len = std::strlen((const char*)p);
    for (int i = 0; i < len; ++i) {
        if (p[i] >= 0xE4 && p[i] <= 0xE9) {
            if (i + 2 < len) {
                unsigned char c1 = p[i], c2 = p[i+1], c3 = p[i+2];
                if ((c1 == 0xE4 && c2 >= 0xB8 && c3 >= 0x80) ||
                    (c1 == 0xE9 && c2 <= 0xBE && c3 <= 0xBF) ||
                    (c1 > 0xE4 && c1 < 0xE9)) {
                    return true;
                }
            }
            i += 2;
        }
    }
    return false;
}

inline bool startsWith(const std::string& str, const std::string& prefix) {
    return str.compare(0, prefix.length(), prefix) == 0;
}

inline bool endsWith(const std::string& str, const std::string& suffix) {
    if (str.length() < suffix.length()) return false;
    return str.compare(str.length() - suffix.length(), suffix.length(), suffix) == 0;
}

// the function is generated by ChatGPT
inline static std::string getCurrentDateTime() {
    std::time_t currentTime = std::time(nullptr);
    std::tm localTime = *std::localtime(&currentTime);

    std::ostringstream oss;
    oss << std::put_time(&localTime, "%A, %B %d, %Y %H:%M:%S");
    return oss.str();
}


inline void replaceAll(std::string &str, const std::string &oldStr, const std::string &newStr) {
    size_t pos = 0;
    while ((pos = str.find(oldStr, pos)) != std::string::npos) {
        str.replace(pos, oldStr.length(), newStr);
        pos += newStr.length();
    }
}


inline std::string&
ltrim(std::string& str)
{
  auto p = std::find_if(
      str.begin(), str.end(), std::not1(std::ptr_fun<int, int>(std::isspace)));
  str.erase(str.begin(), p);
  return str;
}

inline std::string&
rtrim(std::string& str)
{
  auto p = std::find_if(
      str.rbegin(), str.rend(),
      std::not1(std::ptr_fun<int, int>(std::isspace)));
  str.erase(p.base(), str.end());
  return str;
}

inline std::string&
trim(std::string& str)
{
  ltrim(rtrim(str));
  return str;
}

template <typename T>
static T json_value(const json &body, const std::string &key, const T &default_value)
{
    // Fallback null to default value
    return body.contains(key) && !body.at(key).is_null()
        ? body.value(key, default_value)
        : default_value;
}

inline std::string str_replace(const std::string & original, const std::string & search, const std::string & replacement) {
    size_t pos = original.find(search);
    if (pos != std::string::npos) {
        std::string result = original;
        result.replace(pos, search.length(), replacement);
        return result;
    }
    return original;
}

inline std::vector<std::string> str_split(std::string str, const std::string & delimiter) {
    size_t pos = 0;
    std::string token;
    std::vector<std::string> output;
    while ((pos = str.find(delimiter)) != std::string::npos) {
        token = str.substr(0, pos);
        output.push_back(token);
        str.erase(0, pos + delimiter.length());
    }
    output.push_back(str); // the rest
    return output;
}


typedef struct message {
    // "system", "user", "assistant", "tool", Assistant with tool call
    std::string role;
    std::string content;
    std::string name;
    bool has_stop = false;
    size_t tool_calls_cnt = 0;
    json tool_calls;
    message() {}
    message(json oai_json) {
        role = json_value(oai_json, "role", std::string(""));
        content = json_value(oai_json, "content", std::string(""));
        name = json_value(oai_json, "name", std::string(""));
        if (role == "assistant") {
            if (oai_json.contains("tool_calls")) {
                tool_calls_cnt = oai_json["tool_calls"].size();
                tool_calls = oai_json["tool_calls"];
            }
        }
    }
} message;

typedef struct function_param {
    std::string name;
    // type can be "string", "boolean", "number" (typescript types)
    // we do not support array for now
    std::string type;
    std::string desc;
    std::vector<json> allowed_values; // dynamic types
    bool required;
    function_param(std::string param_name, json & oai_json) {
        name = param_name;
        type = json_value(oai_json, "type", std::string());
        desc = json_value(oai_json, "description", std::string());
        if (oai_json.count("enum")) {
            allowed_values = oai_json["enum"];
        }
    }
} function_param;

typedef struct function_def {
    std::string name;
    std::string desc;
    json parameters;
    // parameters.type must always be "object"
    function_def(json & oai_json) {
        std::string type = json_value(oai_json, "type", std::string());
        if (type != "function") {
            throw std::runtime_error("Only tool type \"function\" is supported");
        }
        // function
        json inner_json = json_value(oai_json, "function", json::object());
        name = json_value(inner_json, "name", std::string());
        desc = json_value(inner_json, "description", std::string());
        // function.parameters
        parameters = json_value(inner_json, "parameters", json::object());
    }
} function_def;


inline std::string serialize_function(function_def & fn) {
    std::stringstream ss;
    if (fn.name.empty()) {
        throw std::runtime_error("Function name is empty");
    }

    ss << "### " << fn.name << "\n\n" << fn.name << ": " << fn.desc;
    auto param_str = fn.parameters.dump_style();
    bool has_chinese = contains_chinese(fn.name) || contains_chinese(fn.desc) || contains_chinese(param_str);
    if (has_chinese) {
       ss << " 输入参数：" << param_str << "\n"; 
    } else {
        ss << " Parameters: " << param_str << "\n\n";
    }
    return ss.str();
}

inline static Status generate_oai_message_for_cohere_rag(json & body) {
    const std::string RAG_TAG = "|<documents>|";
    const std::string INSTRUCT_TAG = "|<instruct>|";

    const std::string COMMAND_R_SAFTE_PREAMBLE = "# Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.";
    const std::string COMMAND_R_SYSTEM_PREAMBLE = "# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.";
    const std::string COMMAND_R_USER_PREAMBLE = "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.";
    const std::string COMMAND_R_STYLE_GUIDE = "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.";
    const std::string COMMAND_EXECUTE_INSTRUCT = R"(Carefully perform the following instructions, in order, starting each with a new line.
Firstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.
Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.
Thirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.
Finally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.)";

    std::vector<json> messages = json_value(body, "messages", json::array());
    std::string system_content = json_value(messages[0], "content", std::string());

    std::string user_preamble;
    std::string user_instruct;
    std::string documents_content;
    size_t instructStart = system_content.find(INSTRUCT_TAG);
    size_t documentsStart = system_content.find(RAG_TAG);
    if (instructStart != std::string::npos) {
        user_preamble = system_content.substr(0, instructStart);
        user_instruct = system_content.substr(
            instructStart + INSTRUCT_TAG.length(), 
            documentsStart - instructStart - INSTRUCT_TAG.length());
        documents_content = system_content.substr(documentsStart + RAG_TAG.length());
    } else {
        user_preamble = system_content.substr(0, documentsStart);
        documents_content = system_content.substr(documentsStart + RAG_TAG.length());
        user_instruct = COMMAND_EXECUTE_INSTRUCT;
    }
    if (user_preamble.empty()) {
        user_preamble = "# User Preamble\n## Task and Context\n" + COMMAND_R_USER_PREAMBLE + "\n\n## Style Guide\n" + COMMAND_R_STYLE_GUIDE;
    } else {
        user_preamble = "# User Preamble\n" + user_preamble;
    }

    std::stringstream ss;
    ss << COMMAND_R_SAFTE_PREAMBLE << "\n\n"
       << COMMAND_R_SYSTEM_PREAMBLE << "\n\n"
       << user_preamble;
    std::string global_system_content = ss.str();

    Status status = Status();
    json docs = safe_parse_json(documents_content, status);
    if (!status.ok()) { return status; }

    std::string document_text = "<results>";
    size_t document_index = 0;
    for (const auto& doc: docs) {
        document_text += "\nDocument: " + std::to_string(document_index++);
        for (auto & item: doc.items()) {
            std::string key = item.key();
            std::string value = item.value();
            document_text += "\n" + key + ": " + value;
        }
        document_text += "\n";
    }
    document_text += "<results>";

    json oai_messages = json::array();
    json system_msg = json::object();
    system_msg["role"] = "system";
    system_msg["content"] = global_system_content;
    oai_messages.emplace_back(system_msg);
    
    for (size_t i = 1; i < messages.size(); i++) {
        json msg = json::object();
        msg["role"] = messages[i]["role"];
        msg["content"] = messages[i]["content"];
        oai_messages.emplace_back(msg);
    }
    
    json docs_msg = json::object();
    docs_msg["role"] = "system";
    docs_msg["content"] = document_text;
    oai_messages.emplace_back(docs_msg);

    json instruct_msg = json::object();
    instruct_msg["role"] = "system";
    instruct_msg["content"] = user_instruct;
    oai_messages.emplace_back(instruct_msg);

    body["messages"] = oai_messages;
    return Status();
}

inline static Status generate_oai_message_for_cohere(json & body) {
    // api refer https://docs.cohere.com/reference/chat
    // parameter align
    float p = json_value(body, "p", 0.75);
    body["top_p"] = p;

    int k = json_value(body, "k", 0);
    body["top_k"] = k;

    float temperature = json_value(body, "temperature", 0.3);
    body["temperature"] = temperature;

    auto stop = json_value(body, "stop_sequences", json::array());
    body["stop"] = stop;

    // command r prompt design
    // System Part: Safety Preamble + System Preamble + User Preamble + Tools Def
    const std::string COMMAND_R_SAFTE_PREAMBLE = "# Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral";
    const std::string COMMAND_R_SYSTEM_PREAMBLE = "# System Preamble\n## Basic Rules\nYou are a powerful language agent trained by Cohere to help people. You are capable of complex reasoning and augmented with a number of tools. Your job is to plan and reason about how you will use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see an instruction informing you what kind of response to generate. You will construct a plan and then perform a number of reasoning and action steps to solve the problem. When you have determined the answer to the user's request, you will cite your sources in your answers, according the instructions";
    const std::string COMMAND_R_USER_PREAMBLE = "You use your advanced complex reasoning capabilities to help people by answering their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You may need to use multiple tools in parallel or sequentially to complete your task. You should focus on serving the user's needs as best you can, which will be wide-ranging. The current date is ";
    const std::string COMMAND_R_STYLE_GUIDE = "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling";

    const std::string COMMAND_EXECUTE_INSTRUCT = R"(Carefully perform the following instructions, in order, starting each with a new line.
Firstly, You may need to use complex and advanced reasoning to complete your task and answer the question. Think about how you can use the provided tools to answer the question and come up with a high level plan you will execute.
Write 'Plan:' followed by an initial high level plan of how you will solve the problem including the tools and steps required.
Secondly, Carry out your plan by repeatedly using actions, reasoning over the results, and re-evaluating your plan. Perform Action, Observation, Reflection steps with the following format. Write 'Action:' followed by a json formatted action containing the "tool_name" and "parameters"
 Next you will analyze the 'Observation:', this is the result of the action.
After that you should always think about what to do next. Write 'Reflection:' followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next including if you know the answer to the question.
... (this Action/Observation/Reflection can repeat N times)
Thirdly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.
Fourthly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.
Fifthly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.
Finally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 4>my fact</co: 4> for a fact from document 4.)";

    std::vector<json> messages = json_value(body, "messages", json::array());

    // support cohere rag.
    const std::string RAG_TAG = "|<documents>|";
    if (messages.size() > 0) {
        std::string system_content = json_value(messages[0], "content", std::string());
        if (system_content.find(RAG_TAG) != std::string::npos) {
            return generate_oai_message_for_cohere_rag(body);
        }
    }

    int last_a_index = -1;
    int message_size = (int)messages.size();
    for (int i = message_size - 1; i >= 0; i--) {
        std::string role = json_value(messages[i], "role", std::string());
        if (role.compare("assistant") == 0 && !messages[i].contains("tool_calls")) {
            last_a_index = i;
            break;
        }
    }

    std::vector<message> ua_messages;
    std::vector<message> ft_messages;
    std::string user_preamble = "";
    std::string user_instruct = "";
    std::string system_content = "";
    for (int i = 0; i < message_size; i++) {
        message msg(messages[i]);
        if (msg.role.compare("assistant") == 0) {
            if (msg.tool_calls_cnt > 0) {
                if (i > last_a_index) {
                    ft_messages.emplace_back(msg);
                }
            } else {
                ua_messages.emplace_back(msg);
            }
        }
        if (msg.role.compare("tool") == 0 ) {
            if (i > last_a_index) {
                ft_messages.emplace_back(msg);
            }
        } else if (msg.role.compare("system") == 0) {
            system_content = msg.content;
        } else if (msg.role.compare("user") == 0) {
            ua_messages.emplace_back(msg);
        }
    }

    if (system_content.empty()) {
        std::string ts = getCurrentDateTime();
        user_preamble = "# User Preamble\n## Task and Context\n" + COMMAND_R_USER_PREAMBLE + ts + "\n\n## Style Guide\n" + COMMAND_R_STYLE_GUIDE;
        user_instruct = COMMAND_EXECUTE_INSTRUCT;
    } else {
        const std::string INSTRUCT_TAG = "|<instruct>|";
        size_t pos = system_content.find(INSTRUCT_TAG);
        if (pos != std::string::npos) {
            std::string user_preamble_part = system_content.substr(0, pos);
            if (!user_preamble_part.empty()) {
                user_preamble = "# User Preamble\n" + user_preamble_part;
            } else {
                std::string ts = getCurrentDateTime();
                user_preamble = "# User Preamble\n## Task and Context\n" + COMMAND_R_USER_PREAMBLE + ts + "\n\n## Style Guide\n" + COMMAND_R_STYLE_GUIDE;
            }
            user_instruct = system_content.substr(pos + INSTRUCT_TAG.length());
        } else {
            user_preamble = "# User Preamble\n" + system_content;
            user_instruct = COMMAND_EXECUTE_INSTRUCT;
        }
    }

    // Part I. Tools Def Context
    std::string tools_def_context = "";
    std::vector<json> tools = json_value(body, "tools", json::array());
    // bool has_chinese_prompt = false;
    // for (const auto& msg: ua_messages) {
    //     if (msg.role.compare("user") == 0 && contains_chinese(msg.content)) {
    //         has_chinese_prompt = true;
    //         break;
    //     }
    // }
    // std::string tool_names = "";
    if (!tools.empty()) {
        std::stringstream ss_fn;
        ss_fn << "## Available Tools\nHere is a list of tools that you have available to you:\n\n";

        bool first_output = true;
        for (auto & tool: tools) {
            std::string python_function = "```python\n";
            // Extracting function details from input JSON
            std::string function_name = tool["function"]["name"];
            std::string function_description = tool["function"]["description"];
            json parameters = tool["function"]["parameters"]["properties"];
            
            // Generating Python function signature
            python_function += "def " + function_name + "(";
            for (auto it = parameters.begin(); it != parameters.end(); ++it) {
                std::string type = it.value()["type"].get<std::string>();
                if (type.compare("string") == 0) {
                    type = "str";
                }
                python_function += it.key() + ": " + type + ", ";
            }
            // Removing trailing comma and space
            if (!parameters.empty()) {
                python_function.pop_back(); // remove last comma
                python_function.pop_back(); // remove space
            }
            python_function += ") -> List[Dict]:\n";
            
            // Generating Python function documentation
            python_function += "    \"\"\"" + function_description + "\n";
            if (!parameters.empty()) {
                python_function += "\n    Args:\n";
            }
            
            for (auto it = parameters.begin(); it != parameters.end(); ++it) {
                auto type = it.value()["type"].get<std::string>();
                auto desc = it.value()["description"].get<std::string>();
                if (type.compare("string") == 0) {
                    type = "str";
                }
                python_function += "        " + it.key() + " (" + type + "): " + desc + "\n";
            }
            python_function += "    \"\"\"\n";
            
            // Adding pass statement
            python_function += "    pass\n";
            python_function += "```";
            if (first_output) {
                ss_fn << python_function;
                first_output = false;
            } else {
                ss_fn << "\n\n" << python_function;
            }
        }

        tools_def_context = ss_fn.str();
    }
    
    // Part I. global system message
    std::string global_system_content = "";
    std::stringstream ss;
    if (!tools_def_context.empty()) {
        ss << COMMAND_R_SAFTE_PREAMBLE << "\n\n"
           << COMMAND_R_SYSTEM_PREAMBLE << "\n\n"
           << user_preamble << "\n\n\n";
        
        ss << tools_def_context;

        global_system_content = ss.str();
    
        // add stop sequences for function call
        auto stop = json_value(body, "stop", json::array());
        stop.emplace_back("\nObservation:");
        body["stop"] = stop;
    } else {
        global_system_content = system_content;
    }

    bool has_ft_messages = ft_messages.size() > 0;
    json oai_messages = json::array();
    json system_msg = json::object();
    system_msg["role"] = "system";
    system_msg["content"] = global_system_content;
    oai_messages.emplace_back(system_msg);

    // s: system message
    // U: user message with tool call, f: assistant result with tool call
    // u: user message, t: tool call result
    // support s ua ua U only
    // Part II. user message + Function Result message
    for (size_t i = 0; i < ua_messages.size(); i++) {
        json msg = json::object();
        msg["role"] = ua_messages[i].role;
        msg["content"] = ua_messages[i].content;
        oai_messages.emplace_back(msg);
    }

    // add instruct message for the function call
    if (!tools_def_context.empty()) {
        json msg = json::object();
        msg["role"] = "system";
        msg["content"] = user_instruct;
        oai_messages.emplace_back(msg);
    }

    // add functin call assistant message and result message into the oai_messages
    // the pattern is: f t f t f t a
    // case 1: found the f message repeated for the multiple tool calls in one reflection action
    // case 2: (UNSURE) the first f message (Plan) has mutiple tool calls, the f message should be repeated for each tool call result message
    // maybe: func call logic in the langchain-cohere is not correct
    // in current prompt design, multiple message is not compact, brief, and many repeated tokens
    // very strange. 
    // todo[experiment]: A simple solution is to merge the multiple tool call results into one message internally.

    if (has_ft_messages) {
        // check the tool calls and result count
        bool check_tool_call_results = true;
        for (size_t i = 0; i < ft_messages.size();) {
            message f_msg = ft_messages[i];
            size_t tool_calls_cnt = f_msg.tool_calls.size();
            if (i + tool_calls_cnt >= ft_messages.size()) {
                check_tool_call_results = false;
                break;
            }

            for (size_t j = i + 1; j < i + tool_calls_cnt + 1; j++) {
                message t_msg = ft_messages[j];
                if (t_msg.role.compare("tool") != 0) {
                    check_tool_call_results = false;
                    break;
                }
            }
            i += tool_calls_cnt + 1;
        } 

        if (!check_tool_call_results) {
            return Status(false, "failed to check tool call results count");
        }

        size_t result_cnt = 0;
        Status status = Status();
        for (size_t i = 0; i < ft_messages.size();) {
            message f_msg = ft_messages[i];

            // assitant message with function calls: assistant part + tool calls part
            size_t tool_calls_cnt = f_msg.tool_calls.size();
            json tool_calls = json::array();
            for (size_t k = 0; k < tool_calls_cnt; k++) {
                json tool_call = json::object();
                tool_call["tool_name"] = f_msg.tool_calls[k]["function"]["name"];
                std::string params_str = f_msg.tool_calls[k]["function"]["arguments"];
                json params = safe_parse_json(params_str, status);
                if (!status.ok()) {
                    return Status(false, "failed to parse tool call arguments");
                }

                tool_call["parameters"] = params;
                tool_calls.emplace_back(tool_call);
            }

            json oai_f_msg = json::object();
            oai_f_msg["role"] = "assistant";
            oai_f_msg["content"] = f_msg.content + "\nAction: ```json\n" + tool_calls.dump(4) + "\n```";

            const std::string RAG_RESULT_START_TAG = "[{";
            const std::string RAG_RESULT_END_TAG = "}]";
            for (size_t j = i + 1; j < i + tool_calls_cnt + 1; j++) {
                oai_messages.emplace_back(oai_f_msg);
                message t_msg = ft_messages[j];
                std::string document_text = "<results>";
                if (startsWith(t_msg.content, RAG_RESULT_START_TAG) && endsWith(t_msg.content, RAG_RESULT_END_TAG)) {
                    json func_call_docs = safe_parse_json(t_msg.content, status);
                    if (!status.ok()) {
                        return Status(false, "failed to parse tool call contents");
                    }
        
                    for (auto & doc: func_call_docs) {
                        document_text += "\nDocument: " + std::to_string(result_cnt);
                        result_cnt += 1;
                        for (auto & item: doc.items()) {
                            std::string key = item.key();
                            std::string value = item.value();
                            document_text += "\n" + key + ": " + value;
                        }
                        document_text += "\n";
                    }
                } else {
                    document_text += "\nDocument: " + std::to_string(result_cnt++);
                    document_text += "\nContent: " + t_msg.content + "\n";
                }
                document_text += "</results>";

                json oai_t_msg = json::object();
                oai_t_msg["role"] = "system";
                oai_t_msg["content"] = document_text;
                oai_messages.emplace_back(oai_t_msg);
            }
            i += tool_calls_cnt + 1;
        }
    }

    body["messages"] = oai_messages;

    return Status();
}

///////////////////////////////////////////

inline static Status generate_oai_message_for_qwen15(json & body) {
    // set the default parameters
    if (!body.contains("top_k")) {
        body["top_k"] = 0;
    }

    if (!body.contains("top_p")) {
        body["top_p"] = 0.8;
    }

    if (!body.contains("termperature")) {
        body["termperature"] = 0.85;
    }

    if (!body.contains("repetition_penalty")) {
        body["repetition_penalty"] = 1.1;
    }

    // Qwen Function Call Prompt Design:
    // Part I. system content: system message + Tools Def Context  + Tools Instruct Context
    // Part II. user content: user message + Function Result message
    const std::string QWEN_DEFAULT_SYSTEM = "You are a helpful assistant.";
    const std::string FUNCTIONARY_FN_PROMPT_EN = "# Tools\n\n## You have access to the following tools:\n\n";
    const std::string FUNCTIONARY_FN_PROMPT_ZH = "# 工具\n\n## 你拥有如下工具：\n\n";

    // unsigned char special_token_bytes[3] = {0xe2, 0x9c, 0xbf};
    // std::string special_token(reinterpret_cast<char*>(special_token_bytes), sizeof(special_token_bytes));

    // parse message
    std::vector<json> messages = json_value(body, "messages", json::array());
    // u: user, f: assistant with tool call infomation, t: tool_mesage, a: general assistant messsage
    // U: user message with tool call
    // support case: s ua ua Uftftfta u -> s ua ua Ua u, filter the previous ft messages
    // find the last a message, and mask the previous ft messages
    int last_a_index = -1;
    int message_size = (int)messages.size();
    for (int i = message_size - 1; i >= 0; i--) {
        std::string role = json_value(messages[i], "role", std::string());
        if (role.compare("assistant") == 0 && !messages[i].contains("tool_calls")) {
            last_a_index = i;
            break;
        }
    }

    std::vector<message> ua_messages;
    std::vector<message> ft_messages;
    std::string system_content = "";
    for (int i = 0; i < message_size; i++) {
        message msg(messages[i]);
        if (msg.role.compare("assistant") == 0) {
            if (msg.tool_calls_cnt > 0) {
                if (i > last_a_index) {
                    ft_messages.emplace_back(msg);
                }
            } else {
                ua_messages.emplace_back(msg);
            }
        }
        if (msg.role.compare("tool") == 0 ) {
            if (i > last_a_index) {
                ft_messages.emplace_back(msg);
            }
        } else if (msg.role.compare("system") == 0) {
            system_content = msg.content;
        } else if (msg.role.compare("user") == 0) {
            ua_messages.emplace_back(msg);
        }
    }
    if (system_content.empty()) {
        system_content = QWEN_DEFAULT_SYSTEM;
    }

    // Part I. Tools Def Context
    std::string tools_def_context = "";
    std::vector<json> tools = json_value(body, "tools", json::array());
    bool has_chinese_prompt = false;
    for (const auto& msg: ua_messages) {
        if (msg.role.compare("user") == 0 && contains_chinese(msg.content)) {
            has_chinese_prompt = true;
            break;
        }
    }
    
    std::string tool_names = "";
    if (!tools.empty()) {
        // set stream false for tool calls
        body["stream"] = false;

        std::stringstream ss_fn;
        ss_fn << (has_chinese_prompt ? FUNCTIONARY_FN_PROMPT_ZH : FUNCTIONARY_FN_PROMPT_EN);
        for (auto & tool: tools) {
            function_def fn(tool);
            if (tool_names.empty()) {
                tool_names += fn.name;
            } else {
               tool_names += ("," + fn.name); 
            }
            ss_fn << serialize_function(fn);
        }
        tools_def_context = ss_fn.str();
    }

    // Part I. system message
    std::stringstream ss;
    if (!tools_def_context.empty()) {
        ss << system_content << "\n\n";
        // Part I. join Tools Instruct Context
        ss << tools_def_context << "\n";
        if (!has_chinese_prompt) {
            ss << "## When you need to call a tool, please insert the following command in your reply, which can be called zero or multiple times according to your needs:\n\n";
            ss << "✿FUNCTION✿: The tool to use, should be one of [" << tool_names << "]\n";
            ss << "✿ARGS✿: The input of the tool\n";
            ss << "✿RESULT✿: The result returned by the tool. The image needs to be rendered as ![](url)\n";
            ss << "✿RETURN✿: Reply based on tool result";
        } else {
            ss << "## 你可以在回复中插入零次、一次或多次以下命令以调用工具：\n\n";
            ss << "✿FUNCTION✿: 工具名称，必须是[" << tool_names << "]之一。\n";
            ss << "✿ARGS✿: 工具输入\n";
            ss << "✿RESULT✿: 工具结果，需将图片用![](url)渲染出来。\n";
            ss << "✿RETURN✿: 根据工具结果进行回复";
        }
        system_content = ss.str();
        auto stop = json_value(body, "stop", json::array());
        stop.emplace_back("✿RESULT✿");
        stop.emplace_back("✿RESULT✿:");
        stop.emplace_back("✿RESULT✿:\n");
        body["stop"] = stop;
    }

    ss.str("");
    ss.clear();
    bool has_ft_messages = ft_messages.size() > 0;
    json oai_messages = json::array();
    json system_msg = json::object();
    system_msg["role"] = "system";
    system_msg["content"] = system_content;
    oai_messages.emplace_back(system_msg);

    // s: system message
    // U: user message with tool call, f: assistant result with tool call
    // u: user message, t: tool call result
    // support s ua ua U only
    // Part II. user message + Function Result message
    for (size_t i = 0; i < ua_messages.size() - 1; i++) {
        json msg = json::object();
        msg["role"] = ua_messages[i].role;
        msg["content"] = ua_messages[i].content;
        oai_messages.emplace_back(msg);
    }

    if (has_ft_messages) {
        // Last Message must be use role
        message msg = ua_messages[ua_messages.size() - 1];
        ss << msg.content;
        // check ft_messages
        if (ft_messages.size() % 2 != 0) {
            return Status(false, "ft_messages size is not even");
        }
        ss << "\n\n\n";
        for (size_t i = 0; i < ft_messages.size() - 1; i+=2) {
            message f_msg = ft_messages[i];
            message t_msg = ft_messages[i + 1];
            std::string args_str = f_msg.tool_calls[0]["function"]["arguments"];
            if (i > 0) {
                ss << ":\n";
            }
            ss << "✿FUNCTION✿: " << t_msg.name << "\n";
            ss << "✿ARGS✿: " <<  args_str << "\n";
            ss << "✿RESULT✿: " << t_msg.content << "\n";
            ss << "✿RETURN✿";
        }
        std::string user_content = ss.str();
        json msg_ = json::object();
        msg_["role"] = "user";
        msg_["content"] = user_content;
        oai_messages.emplace_back(msg_);
    } else {
        json msg = json::object();
        msg["role"] = ua_messages[ua_messages.size() - 1].role;
        msg["content"] = ua_messages[ua_messages.size() - 1].content;
        oai_messages.emplace_back(msg);
    }

    body["messages"] = oai_messages;
    return Status();
}


inline static Status convert_oai_response_for_qwen15(json& response) {
    const std::string functionTag = "✿FUNCTION✿:";
    const std::string argsTag = "✿ARGS✿:";

    std::string content = json_value(response["choices"][0]["message"], "content", std::string());

    // Find the function name
    size_t functionNameStart = content.find(functionTag);
    if (functionNameStart == std::string::npos) {
        return Status();
    }
    
    size_t functionNameEnd = content.find('\n', functionNameStart);
    std::string functionName = content.substr(
        functionNameStart + functionTag.length(), 
        functionNameEnd - functionNameStart - functionTag.length());

    // trim the function name
    functionName = trim(functionName);
    if (functionName.empty()) {
        return Status(false, "empty function name");
    }

    // Find the prefix assistant message
    std::string assistant_prefix = content.substr(0, functionNameStart);
    assistant_prefix = trim(assistant_prefix);

    // Find the arguments
    size_t argsStart = content.find(argsTag);
    size_t argsEnd = content.find("}\n", argsStart);
    std::string argsString = content.substr(
        argsStart + argsTag.length(), argsEnd + 1 - argsStart - argsTag.length());
    argsString = trim(argsString);
    if (argsString.empty()) {
        argsString = "{}";
    }

    // special process for the code interpreter arguments, process the argsString
    // Input: ```py\nprint("Hello, World!")\n```
    // Output: {"python_code": "print(\"Hello, World!\")"}
    const std::string pycodeStartTag = "```py\n";
    const std::string pycodeEndTag = "\n```";

    size_t pycodeStart = argsString.find(pycodeStartTag);
    size_t pycodeEnd = argsString.find(pycodeEndTag);
    if (pycodeStart != std::string::npos && pycodeEnd != std::string::npos) {
        std::string pycode = argsString.substr(
            pycodeStart + pycodeStartTag.length(), 
            pycodeEnd - pycodeStart - pycodeStartTag.length());
        replaceAll(pycode, "\n", "\\n");
        argsString = "{\"python_code\": \"" + pycode + "\"}";
    }

    json output = json::object();
    output["role"] = "assistant";
    output["content"] = nullptr;
    output["tool_calls"] = json::array();
    output["tool_calls"].push_back(json::object());
    output["tool_calls"][0]["id"] = "call_" + random_string(25);
    output["tool_calls"][0]["function"] = json::object();
    output["tool_calls"][0]["function"]["name"] = functionName;
    output["tool_calls"][0]["function"]["arguments"] = argsString;
    output["tool_calls"][0]["type"] = "function";
    response["choices"][0]["message"] = output;
    return Status();
}

inline static Status convert_oai_response_for_cohere(json& response) {
    const std::string functionStartTag = "Action: ```json";
    const std::string functionEndTag = "\n```";
    
    std::string content = json_value(response["choices"][0]["message"], "content", std::string());

    // Find the function name
    size_t functionStart = content.find(functionStartTag);
    if (functionStart == std::string::npos) {
        return Status();
    }

    size_t functionEnd = content.find(functionEndTag);
    if (functionEnd == std::string::npos) {
        return Status();
    }

    std::string assistant_content = content.substr(0, functionStart);
    assistant_content = trim(assistant_content);
    std::string functionContent = content.substr(
        functionStart + functionStartTag.length(), 
        functionEnd - functionStart - functionStartTag.length());

    Status status = Status();
    json functionCalls = safe_parse_json(functionContent, status);
    if (!status.ok()) {
        return Status(false, "failed to parse function call");
    }

    json tool_calls = json::array();
    for (const auto& toolCall: functionCalls) {
        json tool_call = json::object();
        tool_call["id"] = "call_" + random_string(25);
        tool_call["function"] = json::object();
        tool_call["function"]["name"] = toolCall["tool_name"];
        tool_call["function"]["arguments"] = toolCall["parameters"].dump_style();
        tool_call["type"] = "function";
        tool_calls.emplace_back(tool_call);
    }

    json output = json::object();
    output["role"] = "assistant";
    output["content"] = assistant_content;
    output["tool_calls"] = tool_calls;
    response["choices"][0]["message"] = output;

    return Status();
}

inline Status generate_oai_messages(json & body) {
    std::string model = json_value(body, "model", std::string());
    std::transform(model.begin(), model.end(), model.begin(),
                [](unsigned char c){ return std::tolower(c); });

    if (model.find("command-r") != std::string::npos) {
        return generate_oai_message_for_cohere(body);
    } else if (model.find("qwen1.5") != std::string::npos) {
        return generate_oai_message_for_qwen15(body);
    } else {
        return Status();
    }
}

inline Status convert_oai_response(json& response) {
    std::string model = json_value(response, "model", std::string());
    std::transform(model.begin(), model.end(), model.begin(),
                [](unsigned char c){ return std::tolower(c); });
    if (model.find("qwen1.5") != std::string::npos)  {
        return convert_oai_response_for_qwen15(response);
    } else if (model.find("command-r") != std::string::npos) {
        return convert_oai_response_for_cohere(response);
    } else {
        return Status();
    }
}

} // namespace llama_functionary